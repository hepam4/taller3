{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 3 MLOps - Data Science Salaries 2023\n",
    "## Fase 2: Preparación de Datos\n",
    "\n",
    "### Universidad EIA\n",
    "Transformación y preparación de datos para modelación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTAR LIBRERÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Librerías importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CARGAR DATOS DEL NOTEBOOK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado: (3755, 11)\n",
      "\n",
      "Primeras filas:\n",
      "   work_year experience_level employment_type                 job_title  \\\n",
      "0       2023               SE              FT  Principal Data Scientist   \n",
      "1       2023               MI              CT               ML Engineer   \n",
      "2       2023               MI              CT               ML Engineer   \n",
      "3       2023               SE              FT            Data Scientist   \n",
      "4       2023               SE              FT            Data Scientist   \n",
      "\n",
      "   salary salary_currency  salary_in_usd employee_residence  remote_ratio  \\\n",
      "0   80000             EUR          85847                 ES           100   \n",
      "1   30000             USD          30000                 US           100   \n",
      "2   25500             USD          25500                 US           100   \n",
      "3  175000             USD         175000                 CA           100   \n",
      "4  120000             USD         120000                 CA           100   \n",
      "\n",
      "  company_location company_size  \n",
      "0               ES            L  \n",
      "1               US            S  \n",
      "2               US            S  \n",
      "3               CA            M  \n",
      "4               CA            M  \n",
      "\n",
      "Columnas: ['work_year', 'experience_level', 'employment_type', 'job_title', 'salary', 'salary_currency', 'salary_in_usd', 'employee_residence', 'remote_ratio', 'company_location', 'company_size']\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos crudos\n",
    "df = pd.read_csv('../data/ds_salaries.csv')\n",
    "\n",
    "print(f\"Dataset cargado: {df.shape}\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "print(df.head())\n",
    "print(f\"\\nColumnas: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EXPLORAR VALORES FALTANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores faltantes:\n",
      "work_year             0\n",
      "experience_level      0\n",
      "employment_type       0\n",
      "job_title             0\n",
      "salary                0\n",
      "salary_currency       0\n",
      "salary_in_usd         0\n",
      "employee_residence    0\n",
      "remote_ratio          0\n",
      "company_location      0\n",
      "company_size          0\n",
      "dtype: int64\n",
      "\n",
      "Sin valores faltantes: True\n"
     ]
    }
   ],
   "source": [
    "# Valores faltantes\n",
    "print(\"Valores faltantes:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nSin valores faltantes: {df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IDENTIFICAR TIPOS DE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables numéricas: ['work_year', 'remote_ratio']\n",
      "Variables categóricas: ['experience_level', 'employment_type', 'job_title', 'company_size', 'company_location']\n",
      "Variable objetivo: salary_in_usd\n",
      "\n",
      "X shape: (3755, 10)\n",
      "y shape: (3755,)\n"
     ]
    }
   ],
   "source": [
    "# Separar variables\n",
    "numerical_features = ['work_year', 'remote_ratio']\n",
    "categorical_features = ['experience_level', 'employment_type', 'job_title', 'company_size', 'company_location']\n",
    "target = 'salary_in_usd'   # ✔️ Corrección clave\n",
    "\n",
    "print(f\"Variables numéricas: {numerical_features}\")\n",
    "print(f\"Variables categóricas: {categorical_features}\")\n",
    "print(f\"Variable objetivo: {target}\")\n",
    "\n",
    "# Separar X (features) y y (target)\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DIVIDIR EN TRAIN, VALIDATION Y TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2253 (60.0%)\n",
      "Val:   751 (20.0%)\n",
      "Test:  751 (20.0%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir en train+val y test (80% - 20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Dividir train+val en train y val (75% - 25% del temp)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Val:   {X_val.shape[0]} ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test:  {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CREAR PIPELINE DE PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline de preprocesamiento creado\n",
      "\n",
      "Datos de train procesados: (2253, 157)\n",
      "Datos de val procesados: (751, 157)\n",
      "Datos de test procesados: (751, 157)\n"
     ]
    }
   ],
   "source": [
    "# Pipeline de transformación\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"✓ Pipeline de preprocesamiento creado\")\n",
    "\n",
    "# Entrenar el pipeline con datos de train\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "print(f\"\\nDatos de train procesados: {X_train_processed.shape}\")\n",
    "\n",
    "# Transformar val y test\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Datos de val procesados: {X_val_processed.shape}\")\n",
    "print(f\"Datos de test procesados: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CONVERTIR A DATAFRAMES (OPCIONAL PARA CLARITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (2253, 157)\n",
      "Train y shape: (2253,)\n"
     ]
    }
   ],
   "source": [
    "# Obtener nombres de features después del OneHotEncoder\n",
    "feature_names = (\n",
    "    numerical_features + \n",
    "    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "# Convertir a DataFrames\n",
    "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names).reset_index(drop=True)\n",
    "X_val_df = pd.DataFrame(X_val_processed, columns=feature_names).reset_index(drop=True)\n",
    "X_test_df = pd.DataFrame(X_test_processed, columns=feature_names).reset_index(drop=True)\n",
    "\n",
    "y_train_reset = y_train.reset_index(drop=True)\n",
    "y_val_reset = y_val.reset_index(drop=True)\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train X shape: {X_train_df.shape}\")\n",
    "print(f\"Train y shape: {y_train_reset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. GUARDAR DATOS PROCESADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos procesados guardados en ../data/processed/\n",
      "Preprocessor guardado en ../models/preprocessor.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Guardar datos procesados\n",
    "X_train_df.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_val_df.to_csv('../data/processed/X_val.csv', index=False)\n",
    "X_test_df.to_csv('../data/processed/X_test.csv', index=False)\n",
    "\n",
    "y_train_reset.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_val_reset.to_csv('../data/processed/y_val.csv', index=False)\n",
    "y_test_reset.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "# Guardar el preprocessor\n",
    "joblib.dump(preprocessor, '../models/preprocessor.pkl')\n",
    "\n",
    "print(\"Datos procesados guardados en ../data/processed/\")\n",
    "print(\"Preprocessor guardado en ../models/preprocessor.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RESUMEN DE LA FASE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESUMEN - FASE 2: PREPARACIÓN DE DATOS\n",
      "======================================================================\n",
      "\n",
      "✓ DATOS ORIGINALES:\n",
      "  Filas: 3755\n",
      "  Columnas: 11\n",
      "\n",
      "✓ DIVISIÓN:\n",
      "  Train: 2253 muestras\n",
      "  Val:   751 muestras\n",
      "  Test:  751 muestras\n",
      "\n",
      "✓ TRANSFORMACIONES:\n",
      "  Variables numéricas: 2 (StandardScaler)\n",
      "  Variables categóricas: 5 (OneHotEncoder)\n",
      "  Features finales: 157\n",
      "\n",
      "✓ ARCHIVOS GENERADOS:\n",
      "  X_train.csv, X_val.csv, X_test.csv\n",
      "  y_train.csv, y_val.csv, y_test.csv\n",
      "  preprocessor.pkl\n",
      "\n",
      "✓ PRÓXIMO PASO:\n",
      "  Ejecutar Notebook 03: Modelación (Optuna + MLFlow)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN - FASE 2: PREPARACIÓN DE DATOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "✓ DATOS ORIGINALES:\n",
    "  Filas: {len(df)}\n",
    "  Columnas: {len(df.columns)}\n",
    "\n",
    "✓ DIVISIÓN:\n",
    "  Train: {len(X_train_df)} muestras\n",
    "  Val:   {len(X_val_df)} muestras\n",
    "  Test:  {len(X_test_df)} muestras\n",
    "\n",
    "✓ TRANSFORMACIONES:\n",
    "  Variables numéricas: {len(numerical_features)} (StandardScaler)\n",
    "  Variables categóricas: {len(categorical_features)} (OneHotEncoder)\n",
    "  Features finales: {len(feature_names)}\n",
    "\n",
    "✓ ARCHIVOS GENERADOS:\n",
    "  X_train.csv, X_val.csv, X_test.csv\n",
    "  y_train.csv, y_val.csv, y_test.csv\n",
    "  preprocessor.pkl\n",
    "\n",
    "✓ PRÓXIMO PASO:\n",
    "  Ejecutar Notebook 03: Modelación (Optuna + MLFlow)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
